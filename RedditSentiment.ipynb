{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedditSentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hx2MOqQmaSGV"
      ],
      "authorship_tag": "ABX9TyMRcajzNY9c36zjLpKMhxnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cullena20/RedditSentiment/blob/main/RedditSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b457c2QEIf-6"
      },
      "source": [
        "# Reddit Sentiment Analysis!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4Y4aLBB8EGk",
        "outputId": "8236b06b-c0eb-4e18-81ed-b13fb32b626c"
      },
      "source": [
        "from IPython import display  # control displaying of printed output in loops\n",
        "import math\n",
        "from pprint import pprint  # pretty print json and lists\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', context='talk', palette='Dark2')\n",
        "!pip install praw\n",
        "import praw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 6.1MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 20.5MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/40/b741437ce4c7b64f928513817b29c0a615efb66ab5e5e01f66fe92d2d95b/prawcore-1.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.17->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2.10)\n",
            "Installing collected packages: update-checker, websocket-client, prawcore, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1hZpf2i8Zwm",
        "outputId": "876b0e7d-2748-46d5-e8a9-d1d3cc0fa59f"
      },
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzeGsbTrIkI2"
      },
      "source": [
        "## Exploring the Reddit API using PRAW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD11pM1p-M1E"
      },
      "source": [
        "Access the Reddit API. To do this go to https://www.reddit.com/prefs/apps/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrvjelK8_Kf"
      },
      "source": [
        "reddit = praw.Reddit(client_id='etCTL0OgGAY1jA',\n",
        "                     client_secret='vMtYIGE5WVK8BDczKh7ZnRup3rb3ew',\n",
        "                     user_agent='Conscious-Reply-7037',\n",
        "                     username='Conscious-Reply-7037')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnC0rmmcFQBU",
        "outputId": "3fd1a361-c3c8-434e-f8ec-20335b29dec4"
      },
      "source": [
        "# subreddit1 = reddit.subreddits.search_by_name('datascience', exact=True)  returns a list of search results\n",
        "subreddit = reddit.subreddit('datascience')\n",
        "print(\"Display Name:\")\n",
        "print(subreddit.display_name) \n",
        "print()\n",
        "print(\"Title:\")\n",
        "print(subreddit.title)   \n",
        "print()\n",
        "print(\"Description\")      \n",
        "print(subreddit.description) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Display Name:\n",
            "datascience\n",
            "\n",
            "Title:\n",
            "Data Science\n",
            "\n",
            "Description\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bJJQ4n4-Skx",
        "outputId": "14522228-ad70-4433-91fb-affd49869076"
      },
      "source": [
        "posts = set()  # use a set to clear any duplicates\n",
        "for post in subreddit.new(limit=None):\n",
        "  posts.add(post)\n",
        "  display.clear_output()  # only one output that changes\n",
        "  print(len(posts))\n",
        "posts = list(posts)  # easier to work with lists"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ScY3aqCJ2Y",
        "outputId": "3a729425-7918-4625-d04a-011a350932af"
      },
      "source": [
        "post = posts[2]\n",
        "print(post.title)\n",
        "print(post.author)\n",
        "print(post.score)\n",
        "print(post.id)\n",
        "print(post.url)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How do I learn more about data cleaning \\ wrangling?\n",
            "Old_H00nter\n",
            "255\n",
            "jlhqq4\n",
            "https://www.reddit.com/r/datascience/comments/jlhqq4/how_do_i_learn_more_about_data_cleaning_wrangling/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx2MOqQmaSGV"
      },
      "source": [
        "## Some Text Preprocessing (unecessary for now)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSm3-lnKc1oS"
      },
      "source": [
        "Preprocess the data by tokenizing it. This is unecessary with the vader model that we are using."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL0zhdT9frvI"
      },
      "source": [
        "We will first use word_tokenize from nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN9Kua6-aVSY",
        "outputId": "3824ae90-34cc-46cc-95ef-0c0476ce7c79"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')  # needed for word_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpnICkufhb6O"
      },
      "source": [
        "Another method to split up article into sentences and them average the positive and negative scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-w4oy3Xgm9O",
        "outputId": "3a885320-04a1-48c9-a8d5-5b1d267905a6"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "article = \"Google staff awoke on Wednesday to surprising news: Their company is working on a search app tailored, and censored, for China. The project, kept secret from all but select teams and leaders, sparked a furious internal debate. Yet the move couldn’t have been entirely surprising for Googlers.\"\n",
        "article_tokenized = sent_tokenize(article)\n",
        "\n",
        "print(article_tokenized)\n",
        "neg_scores = []\n",
        "pos_scores = []\n",
        "for i in article_tokenized:\n",
        "    print(sia.polarity_scores(i))\n",
        "    neg_scores.append(sia.polarity_scores(i)['neg'])\n",
        "    pos_scores.append(sia.polarity_scores(i)['pos'])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Google staff awoke on Wednesday to surprising news: Their company is working on a search app tailored, and censored, for China.', 'The project, kept secret from all but select teams and leaders, sparked a furious internal debate.', 'Yet the move couldn’t have been entirely surprising for Googlers.']\n",
            "{'neg': 0.074, 'neu': 0.829, 'pos': 0.097, 'compound': 0.128}\n",
            "{'neg': 0.265, 'neu': 0.735, 'pos': 0.0, 'compound': -0.7227}\n",
            "{'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compound': 0.3384}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WesNJ7HpgbUJ"
      },
      "source": [
        "I found this code on Github. Let's try it out.\n",
        "Update: word_tokenize works better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxHrEIegfeJ9"
      },
      "source": [
        "# !pip install git+https://github.com/erikavaris/tokenizer.git\n",
        "# from tokenizer import tokenizer\n",
        "# R = tokenizer.RedditTokenizer()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TcYoIrBfDCR",
        "outputId": "cae9c155-f938-41e1-acd6-3e8fd0adfb9a"
      },
      "source": [
        "sentence = \"yo whats up /u/MrJones?! Check out r/spacedicks!\"\n",
        "print(word_tokenize(sentence))\n",
        "# print(R.tokenize(sentence))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yo', 'whats', 'up', '/u/MrJones', '?', '!', 'Check', 'out', 'r/spacedicks', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsXhLQnQIoz0"
      },
      "source": [
        "## Basic Sentiment Analysis Using Pretrained Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMG0DhInVLz9"
      },
      "source": [
        "For now, we will explore various pretrained models that detect negative and positive sentiment. Alternativley, we could train our own model using a dataset and sklearn. However, this will do for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9QabdFhLaxn"
      },
      "source": [
        "Code for other models is commented out because we are not using them. You can uncomment to explore them though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPV6VqnJTQoS"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "sia = SIA()\n",
        "# from textblob import TextBlob\n",
        "# !pip install flair\n",
        "# import flair\n",
        "# flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fx_Qjw8TTcA",
        "outputId": "91544569-fb68-459b-fafe-cfcdce06df50"
      },
      "source": [
        "sentence = \"This food was great but the service was only okay\"\n",
        "print(\"NLTK VADER\")\n",
        "print(sia.polarity_scores(sentence))\n",
        "# print()\n",
        "# print(\"Text Blob:\")\n",
        "# print(TextBlob(sentence).sentiment)\n",
        "# print()\n",
        "# print(\"Flair:\")\n",
        "# s = flair.data.Sentence(sentence)\n",
        "# flair_sentiment.predict(s)\n",
        "# total_sentiment = s.labels\n",
        "# print(total_sentiment)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK VADER\n",
            "{'neg': 0.0, 'neu': 0.62, 'pos': 0.38, 'compound': 0.5994}\n",
            "\n",
            "Text Blob:\n",
            "Sentiment(polarity=0.43333333333333335, subjectivity=0.75)\n",
            "\n",
            "Flair:\n",
            "[POSITIVE (0.5546)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eslGmTm-7By",
        "outputId": "d2df6692-a460-4e62-cdb8-3ed33d6d4f6c"
      },
      "source": [
        "results = list()\n",
        "\n",
        "for post in posts:\n",
        "    pol_score = sia.polarity_scores(post.title)\n",
        "    pol_score['headline'] = post.title\n",
        "    results.append(pol_score)\n",
        "\n",
        "pprint(results[732], width=100)  # pretty print"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'compound': 0.4019,\n",
            " 'headline': 'Any interesting data science/deep learning/machine learning journals?',\n",
            " 'neg': 0.0,\n",
            " 'neu': 0.69,\n",
            " 'pos': 0.31}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZybOo6M-LoB0"
      },
      "source": [
        "Now we will store the data as a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "h4A_SvUX_78G",
        "outputId": "07218cdc-d6b9-4f19-a689-b22d37749b35"
      },
      "source": [
        "df = pd.DataFrame.from_records(results)\n",
        "df['label'] = 0  # creates label column\n",
        "df.loc[df['compound'] > 0.2, 'label'] = 1  # if compound score is greater than 0.2 we label it as positive\n",
        "df.loc[df['compound'] < -0.2, 'label'] = -1  # if compound score is less than -0.2 we label it as positive\n",
        "df.sample(n=10,axis='rows')  # prints 10 random items from the dataframe"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "      <th>headline</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>628</th>\n",
              "      <td>0.14</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>Really Stumped With This Question - Feeling Helpless, Could Someone Please Try to Help? (about network graphs)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Data science position using SAS</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Dummy variables or standardization?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Understanding the Hiring Process (so you can make it work for your)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Actuary to Data scientist.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.758</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.3724</td>\n",
              "      <td>Provocative question: is there a version of R that isn't broken?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.787</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.2263</td>\n",
              "      <td>Tools for documenting data exploration in code repo</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.748</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.4019</td>\n",
              "      <td>My notebooks repo is a hot mess… can y’all help?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Percentage of non traditional path Data Scientists in your team</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.861</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.6369</td>\n",
              "      <td>What’s the best way to build a forecasting prediction model with many (hundreds to a thousand) of relatively short (like 45 days of daily numeric data) time series data?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      neg  ...  label\n",
              "628  0.14  ...      1\n",
              "675  0.00  ...      0\n",
              "18   0.00  ...      0\n",
              "52   0.00  ...      0\n",
              "591  0.00  ...      0\n",
              "61   0.00  ...      1\n",
              "37   0.00  ...      1\n",
              "510  0.00  ...      1\n",
              "484  0.00  ...      0\n",
              "563  0.00  ...      1\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Y4XEgLMDLj"
      },
      "source": [
        "We can explore the most positive and negative sentiments like so. I am using regex to print the headlines a little nicer which seems messy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xIALNGr7SM1",
        "outputId": "71265375-82c7-4395-fea3-a382407f660a"
      },
      "source": [
        "import re\n",
        "pd.options.display.max_colwidth = 200\n",
        "\n",
        "sorted_df = df.sort_values(by='compound')\n",
        "print('Five Most Positive Titles:')\n",
        "print(re.sub(' +', ' ', sorted_df.tail(5)['headline'].to_string(index=False)))  # seems overly complicated\n",
        "print()\n",
        "print('Five Most Negative Titles:')\n",
        "print(re.sub(' +', ' ', sorted_df.head(5)['headline'].to_string(index=False)))  # seems overly complicated\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Five Most Positive Titles:\n",
            " As a data scientist, what action should be taken to have the greatest positive impact on society and the environment?\n",
            " Why do you love / hate about the Data Science field ? And how it compare to Software Engineering. I'm very appreciated about your sharing. Thank you very much.\n",
            " After spending more than a year as a data scientist I found these 4 hard truths data science blogs don't teach you about. I hope sharing my journey helps you in some way.\n",
            " [X-Post] Based on feedback during our Q&A Sessions, I've created a LinkedIn group for aspiring Data Scientists/MLEs that you might find useful. I'm hoping it'll act as a tool to gain useful tips a...\n",
            " A whole year from now to start applying to DS jobs. I'm in a good situation. I can choose what skills to improve, and I'm sure your advice will be super helpful.\n",
            "\n",
            "Five Most Negative Titles:\n",
            " What was your most WTF analysis or insight obtained?\n",
            " Angry rant\n",
            " What is the difference between training error and in-sample error ? (Elements of Statistical Learning)\n",
            " The Future of PyMC3, or: Theano is Dead, Long Live Theano | by PyMC Developers\n",
            " Is DS just becoming a dead end?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwigf8cYIyeQ"
      },
      "source": [
        "Now that we have our results, we can save them in a csv file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HljLKTcKHxZL"
      },
      "source": [
        "result_df = df[['headline', 'label']]\n",
        "result_df.to_csv('reddit_headline_sentiment.csv', mode='a', encoding='UTF-8', index=False)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awbBITlWI4B_"
      },
      "source": [
        "## Exploring Our Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfNPwecTMr5H"
      },
      "source": [
        "This code will print th first five negative results and the first five positive results. These do not take into account how positive or negative that they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcXJMJu_JaYf",
        "outputId": "8df7f96c-1952-4b6e-cacd-3a9e740042d8"
      },
      "source": [
        "positive_results = df[df['label'] == 1]\n",
        "negative_results = df[df['label'] == -1]\n",
        "print(\"Postitive Results:\")\n",
        "pprint(list(positive_results['headline'])[:5]) \n",
        "print()\n",
        "print(\"Negative Results:\")\n",
        "pprint(list(negative_results['headline'])[:5]) "
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Postitive Results:\n",
            "['Best Certificates for Data Science',\n",
            " 'Project ideas that combines data science with Business Intelligence or '\n",
            " 'Business Analytics for my thesis',\n",
            " 't-SNE usefulness',\n",
            " 'Data Science Degree/Career on the line, please help!',\n",
            " 'Potential of Artificial Intelligence (AI) in Military']\n",
            "\n",
            "Negative Results:\n",
            "['What are the common mistakes aspiring data scientists make?',\n",
            " 'Is putting data camp certificates on linkedin obnoxious?',\n",
            " 'Question about aftereffects of predictive modeling during a crisis',\n",
            " 'Landing a Senior Data Scientist Job After 6 Months of Unemployment',\n",
            " 'Job scams']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tuel0lDlKUnh",
        "outputId": "8701c4df-6d8b-44dc-f225-e604a2aabbef"
      },
      "source": [
        "percentages = df.label.value_counts(normalize=True) * 100\n",
        "print(\"Count:\")\n",
        "print(df.label.value_counts())\n",
        "print()\n",
        "print(\"Percentages:\")\n",
        "print(percentages)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count:\n",
            " 0    458\n",
            " 1    206\n",
            "-1     71\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Percentages:\n",
            " 0    62.312925\n",
            " 1    28.027211\n",
            "-1     9.659864\n",
            "Name: label, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "H74CRUr-Ktpy",
        "outputId": "056b7f65-9481-46f2-e882-78eeb9a8df9a"
      },
      "source": [
        "sns.barplot(x=percentages.index, y=percentages)\n",
        "plt.xlabel = ['Negative', 'Nuetral', 'Positive']\n",
        "plt.plot()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-XSijy2ixGW"
      },
      "source": [
        "## More with the Reddit API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJbxf5QPj1_D"
      },
      "source": [
        "Let's take this a bit further and explore comments in posts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iahy4DkiizcT",
        "outputId": "b3def1a8-c57f-4c75-c65e-35ff5f5f4495"
      },
      "source": [
        "# this creates a list of comments from the post we already defined\n",
        "comments = list(post.comments)\n",
        "# pprint(vars(comments[1]))  # gives us variables for comment\n",
        "print('Post Title:', post.title)\n",
        "print()\n",
        "print('Comment: ', comments[1].body)\n",
        "print()\n",
        "print('Comment Author: ', comments[1].author)\n",
        "print('Score: ', comments[1].score)  # would be nice to have model weigh this too"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post Title: Linking Script/Report Output to PPT Presentations - Best Practices?\n",
            "\n",
            "Comment:  You can add linked objects (tables, graphs, etc) in PPT, which will update their values to match the objects in another file, like Excel.\n",
            "\n",
            "But then you have to keep the excel updated.\n",
            "\n",
            "And you have to worry about file paths matching and using network share drives.\n",
            "\n",
            "You could try automating the population of that excel object, but that's super brittle and to be honest you'd probably spend more hours maintaining that code with all the changes than it would to just keep up with the PowerPoint.\n",
            "\n",
            "Comment Author:  playawhile\n",
            "Score:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Rxqh9ZNaog"
      },
      "source": [
        "Perhaps we can make a model that gathers sentiment from the comments within a subreddit as well as from the top posts."
      ]
    }
  ]
}